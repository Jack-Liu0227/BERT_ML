# BERT-Based Material Property Prediction / åŸºäº BERT çš„ææ–™æ€§èƒ½é¢„æµ‹

This repository contains code and resources for predicting material properties using advanced language models like MatSciBERT, SciBERT, and SteelBERT. The project focuses on various alloys including Titanium, Aluminum, Steel, and High-Entropy Alloys (HEAs).

> **Navigation**:
>
> - [ğŸ‡¬ğŸ‡§ English Documentation](#english-documentation)
> - [ğŸ‡¨ğŸ‡³ ä¸­æ–‡è¯´æ˜ (Chinese Documentation)](#chinese-documentation)

---

<div id="english-documentation"></div>

## ğŸ‡¬ğŸ‡§ English Documentation

### ğŸ“– Introduction

This project leverages pre-trained BERT models adapted for material science to predict key mechanical properties such as Yield Strength (YS), Ultimate Tensile Strength (UTS), and Elongation (EL). It provides a complete pipeline from data preprocessing and feature engineering to model training and visualization.

### ğŸ› ï¸ Environment Setup & Reproduction

To ensure that you can fully reproduce our results, we have exported the exact environment configurations. You can set up the environment using either Conda or Pip.

#### Prerequisites
- Anaconda or Miniconda (recommended)
- Python (as specified in `environment.yml`)

#### Method 1: Using Conda (Recommended)
This method ensures that all binary dependencies and specific versions are installed correctly.

```bash
# Create the environment from the file
conda env create -f environment.yml

# Activate the environment
conda activate llm
```

#### Method 2: Using Pip
If you prefer a standard pip workflow:

```bash
pip install -r requirements.txt
```

### ğŸ“¥ Model Preparation

Before running the training or prediction scripts, you need to download the pre-trained BERT models (MatSciBERT, SciBERT, SteelBERT). We provided a script to automate this process.

```bash
# Download all required models
python models/download_bert_models.py
```

> **Note**: **SteelBERT** is a gated repository on Hugging Face. You must:
> 1. Run `huggingface-cli login` to authenticate.
> 2. Request access at [https://huggingface.co/MGE-LLMs/SteelBERT](https://huggingface.co/MGE-LLMs/SteelBERT).

### ğŸ“‚ Project Structure

- **`models/`**: Stores pre-trained models (MatSciBERT, SciBERT, etc.).
- **`datasets/`**: Raw and processed datasets for different alloy systems.
- **`src/`**: Source code for core logic (training, prediction).
    - **`src/pipelines/`**: Contains batch processing configs and scripts.
- **`Scripts/`**: Utility scripts for plotting and batch processing.
- **`output/`**: Directory where results and logs are saved.

### ğŸš€ Usage

The project provides a powerful batch processing system to run experiments across multiple alloy types and models.

#### Batch Execution (`src.pipelines.run_batch`)

The main entry point for running experiments is `src.pipelines.run_batch`. It supports:
- Running predefined experimental configurations.
- Running custom combinations of models and alloys.
- Resuming interrupted runs.
- Parallel execution.

**1. Basic Commands**

```bash
# List all available configurations and alloy types
python -m src.pipelines.run_batch --list

# Run a specific configuration (e.g., matching traditional ML models)
python -m src.pipelines.run_batch --config experiment1_all_ml_models

# Run multiple configurations
python -m src.pipelines.run_batch --config experiment1_all_ml_models experiment2a_all_nn_scibert
```

**2. Running Complete Experiments**

We have defined comprehensive experiment sets (Experiment 1 & 2):

```bash
# Run ALL experiments (Experiment 1 + Experiment 2a/2b/2c)
python -m src.pipelines.run_batch --all

# Run only Experiment 1 (Traditional ML Models: XGBoost, RF, MLP, etc.)
python -m src.pipelines.run_batch --experiment1

# Run only Experiment 2 (Neural Networks with BERT Embeddings)
python -m src.pipelines.run_batch --experiment2
```

**3. Custom Execution**

You can also run custom setups without using predefined configs:

```bash
# Run SteelBERT encoding for all alloys using Neural Networks
python -m src.pipelines.run_batch --custom \
    --embedding_type steelbert \
    --use_nn \
    --epochs 100

# Run specific traditional models with custom settings
python -m src.pipelines.run_batch --custom \
    --embedding_type tradition \
    --models xgboost lightgbm \
    --use_composition_feature
```

**4. Advanced Features**

- **Preview (Dry Run)**: See what commands will be executed without running them.
  ```bash
  python -m src.pipelines.run_batch --all --dry_run
  ```

- **Resume Support**: If a task is interrupted, use `--resume` to continue from where it left off.
  ```bash
  python -m src.pipelines.run_batch --all --resume
  ```

- **Progress Management**:
  ```bash
  # View current progress
  python -m src.pipelines.run_batch --show_progress
  
  # Clear progress history
  python -m src.pipelines.run_batch --clear_progress
  ```

#### Configuration Files

- **`src/pipelines/batch_configs.py`**:
  - `ALLOY_CONFIGS`: Defines dataset paths, target columns, and processing parameters for each alloy (Ti, Al, HEA, Steel, etc.).
  - `BATCH_CONFIGS`: Defines experiment parameters (models to use, validation settings, hyperparameters).

---

<div id="chinese-documentation"></div>

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡è¯´æ˜ (Chinese Documentation)

[â¬†ï¸ Back to Top / å›åˆ°é¡¶éƒ¨](#bert-based-material-property-prediction--åŸºäº-bert-çš„ææ–™æ€§èƒ½é¢„æµ‹)

### ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®åˆ©ç”¨é’ˆå¯¹ææ–™ç§‘å­¦é¢„è®­ç»ƒçš„ BERT æ¨¡å‹ï¼ˆå¦‚ MatSciBERT, SciBERT, SteelBERTï¼‰æ¥é¢„æµ‹å…³é”®çš„æœºæ¢°æ€§èƒ½ï¼Œä¾‹å¦‚å±ˆæœå¼ºåº¦ (YS)ã€ç”±äºæŠ—æ‹‰å¼ºåº¦ (UTS) å’Œå»¶ä¼¸ç‡ (EL)ã€‚ä¸»è¦ç ”ç©¶å¯¹è±¡æ¶µç›–é’›åˆé‡‘ã€é“åˆé‡‘ã€é’¢æä»¥åŠé«˜ç†µåˆé‡‘ (HEAs)ã€‚

### ğŸ› ï¸ ç¯å¢ƒå®‰è£…ä¸å¤ç°

ä¸ºäº†ç¡®ä¿æ‚¨å¯ä»¥å®Œå…¨å¤ç°æˆ‘ä»¬çš„é¡¹ç›®ç»“æœï¼Œæˆ‘ä»¬å¯¼å‡ºäº†è¯¦ç»†çš„ç¯å¢ƒé…ç½®æ–‡ä»¶ã€‚æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨ Conda æˆ– Pip è¿›è¡Œå®‰è£…ã€‚

#### å‰ç½®è¦æ±‚
- Anaconda æˆ– Miniconda (æ¨è)
- Python (ç‰ˆæœ¬è¯¦è§ `environment.yml`)

#### æ–¹æ³• 1: ä½¿ç”¨ Conda (æ¨è)
æ­¤æ–¹æ³•èƒ½æœ€å‡†ç¡®åœ°è¿˜åŸè¿è¡Œç¯å¢ƒï¼ŒåŒ…å«æ‰€æœ‰ä¾èµ–åº“ã€‚

```bash
# ä» yaml æ–‡ä»¶åˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml

# æ¿€æ´»ç¯å¢ƒ
conda activate llm
```

#### æ–¹æ³• 2: ä½¿ç”¨ Pip
å¦‚æœæ‚¨æ›´ä¹ æƒ¯ä½¿ç”¨ pipï¼š

```bash
pip install -r requirements.txt
```

### ğŸ“¥ æ¨¡å‹å‡†å¤‡

åœ¨å¼€å§‹è®­ç»ƒæˆ–é¢„æµ‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦ä¸‹è½½é¢„è®­ç»ƒçš„ BERT æ¨¡å‹ (MatSciBERT, SciBERT, SteelBERT)ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è„šæœ¬æ¥å®Œæˆæ­¤æ­¥éª¤ã€‚

```bash
# ä¸‹è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹
python models/download_bert_models.py
```

> **æ³¨æ„**: **SteelBERT** æ˜¯ Hugging Face ä¸Šçš„å—æ§ä»“åº“ã€‚æ‚¨å¿…é¡»ï¼š
> 1. è¿è¡Œ `huggingface-cli login` è¿›è¡Œç™»å½•ã€‚
> 2. åœ¨ [https://huggingface.co/MGE-LLMs/SteelBERT](https://huggingface.co/MGE-LLMs/SteelBERT) ç”³è¯·è®¿é—®æƒé™ã€‚

### ğŸ“‚ é¡¹ç›®ç»“æ„

- **`models/`**: å­˜æ”¾é¢„è®­ç»ƒæ¨¡å‹ (MatSciBERT, SciBERT ç­‰)ã€‚
- **`datasets/`**: ä¸åŒåˆé‡‘ä½“ç³»çš„åŸå§‹åŠå¤„ç†åçš„æ•°æ®é›†ã€‚
- **`src/`**: æ ¸å¿ƒæºä»£ç  (è®­ç»ƒã€é¢„æµ‹é€»è¾‘)ã€‚
    - **`src/pipelines/`**: åŒ…å«æ‰¹é‡å¤„ç†çš„é…ç½®å’Œè„šæœ¬ã€‚
- **`Scripts/`**: ç”¨äºç»˜å›¾å’Œæ‰¹å¤„ç†çš„å·¥å…·è„šæœ¬ã€‚
- **`output/`**: ç»“æœå’Œæ—¥å¿—è¾“å‡ºç›®å½•ã€‚

### ğŸš€ ä½¿ç”¨è¯´æ˜

æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ‰¹é‡å¤„ç†ç³»ç»Ÿï¼Œå¯ä»¥é’ˆå¯¹å¤šç§åˆé‡‘ç±»å‹å’Œæ¨¡å‹è¿è¡Œå®éªŒã€‚

#### æ‰¹é‡æ‰§è¡Œ (`src.pipelines.run_batch`)

è¿è¡Œå®éªŒçš„ä¸»è¦å…¥å£æ˜¯ `src.pipelines.run_batch`ã€‚å®ƒæ”¯æŒï¼š
- è¿è¡Œé¢„å®šä¹‰çš„å®éªŒé…ç½®ã€‚
- è¿è¡Œè‡ªå®šä¹‰çš„æ¨¡å‹å’Œåˆé‡‘ç»„åˆã€‚
- æ–­ç‚¹ç»­ä¼ ï¼ˆä»ä¸­æ–­å¤„ç»§ç»­è¿è¡Œï¼‰ã€‚
- å¹¶è¡Œæ‰§è¡Œã€‚

**1. åŸºç¡€å‘½ä»¤**

```bash
# åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®å’Œåˆé‡‘ç±»å‹
python -m src.pipelines.run_batch --list

# è¿è¡Œç‰¹å®šé…ç½®ï¼ˆä¾‹å¦‚ï¼šä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ¯”ï¼‰
python -m src.pipelines.run_batch --config experiment1_all_ml_models

# åŒæ—¶è¿è¡Œå¤šä¸ªé…ç½®
python -m src.pipelines.run_batch --config experiment1_all_ml_models experiment2a_all_nn_scibert
```

**2. è¿è¡Œå®Œæ•´å®éªŒ**

æˆ‘ä»¬å®šä¹‰äº†å®Œæ•´çš„å®éªŒé›†ï¼ˆå®éªŒ 1 å’Œ å®éªŒ 2ï¼‰ï¼š

```bash
# è¿è¡Œæ‰€æœ‰å®éªŒï¼ˆå®éªŒ 1 + å®éªŒ 2a/2b/2cï¼‰
python -m src.pipelines.run_batch --all

# ä»…è¿è¡Œå®éªŒ 1ï¼ˆä¼ ç»Ÿ ML æ¨¡å‹ï¼šXGBoost, RF, MLP ç­‰ï¼‰
python -m src.pipelines.run_batch --experiment1

# ä»…è¿è¡Œå®éªŒ 2ï¼ˆç¥ç»ç½‘ç»œ + BERT åµŒå…¥ï¼‰
python -m src.pipelines.run_batch --experiment2
```

**3. è‡ªå®šä¹‰è¿è¡Œ**

æ‚¨ä¹Ÿå¯ä»¥åœ¨ä¸ä½¿ç”¨é¢„å®šä¹‰é…ç½®çš„æƒ…å†µä¸‹è¿›è¡Œè‡ªå®šä¹‰è®¾ç½®ï¼š

```bash
# ä½¿ç”¨ç¥ç»ç½‘ç»œè¿è¡Œ SteelBERT ç¼–ç ï¼ˆé’ˆå¯¹æ‰€æœ‰åˆé‡‘ï¼‰
python -m src.pipelines.run_batch --custom \
    --embedding_type steelbert \
    --use_nn \
    --epochs 100

# è¿è¡Œç‰¹å®šçš„ä¼ ç»Ÿæ¨¡å‹å¹¶ä½¿ç”¨è‡ªå®šä¹‰è®¾ç½®
python -m src.pipelines.run_batch --custom \
    --embedding_type tradition \
    --models xgboost lightgbm \
    --use_composition_feature
```

**4. é«˜çº§åŠŸèƒ½**

- **é¢„è§ˆæ¨¡å¼ (Dry Run)**: æŸ¥çœ‹å°†è¦æ‰§è¡Œçš„å‘½ä»¤è€Œä¸å®é™…è¿è¡Œã€‚
  ```bash
  python -m src.pipelines.run_batch --all --dry_run
  ```

- **æ–­ç‚¹ç»­ä¼ **: å¦‚æœä»»åŠ¡ä¸­æ–­ï¼Œä½¿ç”¨ `--resume` ä»ä¸­æ–­å¤„ç»§ç»­æ‰§è¡Œã€‚
  ```bash
  python -m src.pipelines.run_batch --all --resume
  ```

- **è¿›åº¦ç®¡ç†**:
  ```bash
  # æŸ¥çœ‹å½“å‰è¿›åº¦
  python -m src.pipelines.run_batch --show_progress
  
  # æ¸…é™¤è¿›åº¦è®°å½•
  python -m src.pipelines.run_batch --clear_progress
  ```

#### é…ç½®æ–‡ä»¶

- **`src/pipelines/batch_configs.py`**:
  - `ALLOY_CONFIGS`: å®šä¹‰æ¯ç§åˆé‡‘ï¼ˆTi, Al, HEA, Steel ç­‰ï¼‰çš„æ•°æ®é›†è·¯å¾„ã€ç›®æ ‡åˆ—å’Œå¤„ç†å‚æ•°ã€‚
  - `BATCH_CONFIGS`: å®šä¹‰å®éªŒå‚æ•°ï¼ˆä½¿ç”¨çš„æ¨¡å‹ã€éªŒè¯è®¾ç½®ã€è¶…å‚æ•°ï¼‰ã€‚
