"""
æ¨¡å‹å¯¹æ¯”å™¨æ¨¡å—
Model Comparator Module

æä¾›å¤šæ¨¡å‹å¯¹æ¯”çš„æ ¸å¿ƒåŠŸèƒ½
Provides core functionality for multi-model comparison
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any
from pathlib import Path

# è®¾ç½®æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯ä¸ºUTF-8ç¼–ç 
# Set stdout and stderr to UTF-8 encoding
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    sys.stderr.reconfigure(encoding='utf-8')

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆå½“ä½œä¸ºæ¨¡å—è¿è¡Œæ—¶ï¼‰
    from .pipeline import MLTrainingPipeline
    from .utils import to_long_path
    from .model_comparison_plots import create_model_comparison_plots
except ImportError:
    # å°è¯•ç›´æ¥å¯¼å…¥ï¼ˆå½“ç›´æ¥è¿è¡Œæ—¶ï¼‰
    try:
        from pipeline import MLTrainingPipeline
        from utils import to_long_path
        from model_comparison_plots import create_model_comparison_plots
    except ImportError:
        # ä½¿ç”¨å®Œæ•´è·¯å¾„å¯¼å…¥
        from src.models.ml_train_eval_pipeline.pipeline import MLTrainingPipeline
        from src.models.ml_train_eval_pipeline.utils import to_long_path
        from src.models.ml_train_eval_pipeline.model_comparison_plots import create_model_comparison_plots


class ModelComparator:
    """
    å¤šæ¨¡å‹å¯¹æ¯”å™¨
    Multi-Model Comparator
    """
    
    def __init__(self, base_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ¨¡å‹å¯¹æ¯”å™¨
        Initialize model comparator
        
        Args:
            base_config: åŸºç¡€é…ç½®å­—å…¸
        """
        self.base_config = base_config
        self.results = {}
        self.comparison_dir = None
        
    def compare_models(self, 
                      models_to_compare: List[str], 
                      use_optuna: bool = False,
                      n_trials: int = 50,
                      n_repeats: int = 1) -> Dict[str, Any]:
        """
        å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
        Compare performance of multiple models
        """
        print("=" * 80)
        print("å¼€å§‹å¤šæ¨¡å‹å¯¹æ¯” / Starting Multi-Model Comparison")
        print("=" * 80)
        
        # åˆ›å»ºå¯¹æ¯”ç»“æœç›®å½•
        base_result_dir = self.base_config['result_dir']
        self.comparison_dir = os.path.join(base_result_dir, "model_comparison")
        os.makedirs(self.comparison_dir, exist_ok=True)
        
        for model_type in models_to_compare:
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒæ¨¡å‹ / Training Model: {model_type.upper()}")
            print(f"{'='*60}")
            
            try:
                # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºåŸºç¡€ç»“æœç›®å½•
                model_base_dir = os.path.join(self.comparison_dir, f"{model_type}_results")
                
                repeat_results = []
                
                for i in range(n_repeats):
                    if n_repeats > 1:
                        print(f"\n  >> è¿è¡Œé‡å¤å®éªŒ {i+1}/{n_repeats} (Run {i+1}/{n_repeats})")
                        model_result_dir = os.path.join(model_base_dir, f"repeat_{i}")
                    else:
                        model_result_dir = model_base_dir
                    
                    # åˆ›å»ºæ¨¡å‹ç‰¹å®šçš„é…ç½®
                    model_config = self.base_config.copy()
                    model_config['model_type'] = model_type
                    model_config['result_dir'] = model_result_dir
                    model_config['use_optuna'] = use_optuna
                    model_config['n_trials'] = n_trials
                    
                    # æ›´æ–°éšæœºç§å­ä»¥ä¿è¯æ¯æ¬¡è¿è¡Œä¸åŒ
                    if 'random_state' in model_config:
                        model_config['random_state'] = model_config['random_state'] + i
                    
                    # è®­ç»ƒæ¨¡å‹
                    result = self._train_single_model(model_config)
                    repeat_results.append(result)
                
                # å¦‚æœæœ‰å¤šæ¬¡é‡å¤ï¼Œèšåˆç»“æœ
                if n_repeats > 1:
                    print(f"\n[èšåˆ] è®¡ç®— {n_repeats} æ¬¡è¿è¡Œçš„å¹³å‡æŒ‡æ ‡...")
                    final_result = self._aggregate_repeat_results(repeat_results)
                    self.results[model_type] = final_result
                else:
                    self.results[model_type] = repeat_results[0]

                print(f"[OK] æ¨¡å‹ {model_type} è®­ç»ƒå®Œæˆ")

            except Exception as e:
                print(f"[FAIL] æ¨¡å‹ {model_type} è®­ç»ƒå¤±è´¥: {str(e)}")
                self.results[model_type] = {'error': str(e)}
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_comparison_report()
        
        return self.results
    
    def _train_single_model(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„å‘½ä»¤è¡Œå‚æ•°
        class Args:
            def __init__(self, config_dict):
                # è®¾ç½®æ‰€æœ‰å¿…éœ€çš„é»˜è®¤å±æ€§
                self.data_file = None
                self.result_dir = None
                self.model_type = 'xgboost'
                self.target_columns = []
                self.processing_cols = []
                self.use_composition_feature = False
                self.use_temperature = False
                self.other_features_name = None
                self.test_size = 0.2
                self.random_state = 42
                self.evaluate_after_train = True
                self.run_shap_analysis = False
                self.cross_validate = True
                self.num_folds = 5
                self.use_optuna = False
                self.n_trials = 50
                self.study_name = 'ml_hyperparameter_optimization'
                self.mlp_max_iter = 200

                # ç”¨é…ç½®å­—å…¸ä¸­çš„å€¼è¦†ç›–é»˜è®¤å€¼
                for key, value in config_dict.items():
                    setattr(self, key, value)

        args = Args(config)
        
        # å¤„ç†Windowsé•¿è·¯å¾„
        args.result_dir = to_long_path(args.result_dir)
        
        # åˆ›å»ºå¹¶è¿è¡Œç®¡é“
        pipeline = MLTrainingPipeline(args)
        pipeline.run()
        
        # æ”¶é›†ç»“æœ
        result = self._collect_model_results(args.result_dir)
        return result
    
    def _aggregate_repeat_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èšåˆå¤šæ¬¡é‡å¤å®éªŒçš„ç»“æœ"""
        aggregated = {}
        
        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡
        all_metrics = {}
        valid_results_count = 0
        
        for res in results:
            if 'error' in res:
                continue
                
            valid_results_count += 1
            
            # ä»final_evaluationæå–
            if 'final_evaluation' in res:
                metrics = res['final_evaluation']
                for k, v in metrics.items():
                    # åªèšåˆæ•°å€¼å‹æŒ‡æ ‡
                    if isinstance(v, (int, float)):
                        if k not in all_metrics:
                            all_metrics[k] = []
                        all_metrics[k].append(v)
            
            # ä»evaluationæå–(å¤‡ç”¨)
            elif 'evaluation' in res and 'test_metrics' in res['evaluation']:
                test_metrics = res['evaluation']['test_metrics']
                for target, metrics in test_metrics.items():
                    for m_name, m_val in metrics.items():
                        key = f"test_{target}_{m_name}"
                        if key not in all_metrics:
                            all_metrics[k] = []
                        all_metrics[k].append(m_val)

        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        final_evaluation = {}
        for k, values in all_metrics.items():
            if values:
                final_evaluation[k] = float(np.mean(values))
                final_evaluation[f"{k}_std"] = float(np.std(values))
                final_evaluation[f"{k}_all_values"] = values
        
        aggregated['final_evaluation'] = final_evaluation
        aggregated['repeat_results'] = results
        aggregated['n_repeats'] = len(results)
        aggregated['valid_repeats'] = valid_results_count
        
        return aggregated

    def _collect_model_results(self, result_dir: str) -> Dict[str, Any]:
        """æ”¶é›†æ¨¡å‹è®­ç»ƒç»“æœ"""
        result = {}

        try:
            # è¯»å–æœ€ç»ˆè¯„ä¼°ç»“æœ
            final_eval_file = os.path.join(result_dir, "final_evaluation_metrics.json")
            if os.path.exists(final_eval_file):
                with open(final_eval_file, 'r', encoding='utf-8') as f:
                    final_metrics = json.load(f)
                    result['final_evaluation'] = final_metrics

            # è¯»å–è¯„ä¼°ç»“æœï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            eval_file = os.path.join(result_dir, "evaluation_results.json")
            if os.path.exists(eval_file):
                with open(eval_file, 'r', encoding='utf-8') as f:
                    result['evaluation'] = json.load(f)

            # è¯»å–äº¤å‰éªŒè¯ç»“æœ
            cv_file = os.path.join(result_dir, "cross_validation_results.json")
            if os.path.exists(cv_file):
                with open(cv_file, 'r', encoding='utf-8') as f:
                    result['cross_validation'] = json.load(f)

            # è¯»å–äº¤å‰éªŒè¯å¹³å‡æŒ‡æ ‡ï¼ˆæ–°æ ¼å¼ï¼‰
            cv_avg_file = os.path.join(result_dir, "cv_avg_metrics.json")
            if os.path.exists(cv_avg_file):
                with open(cv_avg_file, 'r', encoding='utf-8') as f:
                    cv_avg_metrics = json.load(f)
                    result['cv_avg_metrics'] = cv_avg_metrics

            # è¯»å–Optunaæœ€ä½³å‚æ•°
            optuna_file = os.path.join(result_dir, "optuna_best_params.json")
            if os.path.exists(optuna_file):
                with open(optuna_file, 'r', encoding='utf-8') as f:
                    result['best_params'] = json.load(f)

            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_file = os.path.join(result_dir, "best_model.pkl")
            result['model_saved'] = os.path.exists(model_file)

        except Exception as e:
            result['collection_error'] = str(e)

        return result
    
    def _generate_comparison_report(self):
        """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š / Generating Comparison Report")
        print(f"{'='*80}")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        
        for model_name, result in self.results.items():
            if 'error' in result:
                continue
                
            row = {'Model': model_name}

            # Flag to track if we found test set results
            found_test_results = False

            # ä¼˜å…ˆæå–æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°æŒ‡æ ‡ (Priority 1: Final test set evaluation)
            if 'final_evaluation' in result:
                final_metrics = result['final_evaluation']
                for metric_key, metric_value in final_metrics.items():
                    if '_test_' in metric_key:
                        parts = metric_key.split('_')
                        if len(parts) >= 4:
                            target_start_idx = parts.index('test') + 1
                            metric_type = parts[-1].upper()
                            target_name = '_'.join(parts[target_start_idx:-1])
                            row[f'{target_name}_{metric_type}'] = metric_value
                            found_test_results = True

            # å¤‡é€‰ï¼šæå–è¯„ä¼°æŒ‡æ ‡ä¸­çš„æµ‹è¯•é›†ç»“æœ (Priority 2: Test metrics from evaluation)
            if not found_test_results and 'evaluation' in result:
                eval_data = result['evaluation']
                if 'test_metrics' in eval_data:
                    for target, metrics in eval_data['test_metrics'].items():
                        row[f'{target}_R2'] = metrics.get('r2', 'N/A')
                        row[f'{target}_RMSE'] = metrics.get('rmse', 'N/A')
                        row[f'{target}_MAE'] = metrics.get('mae', 'N/A')
                        found_test_results = True

            # æœ€åå¤‡é€‰ï¼šä½¿ç”¨äº¤å‰éªŒè¯å¹³å‡ç»“æœ (Priority 3: CV average as fallback)
            if not found_test_results and 'cv_avg_metrics' in result:
                cv_avg = result['cv_avg_metrics']
                # Extract target-specific metrics from CV averages
                for key, value in cv_avg.items():
                    if '_' in key:  # Target-specific metrics like "UTS(MPa)_r2"
                        parts = key.split('_')
                        if len(parts) >= 2:
                            target_name = '_'.join(parts[:-1])
                            metric_type = parts[-1].upper()
                            row[f'{target_name}_{metric_type}'] = value
                            found_test_results = True

            # è®°å½•äº¤å‰éªŒè¯ç»“æœç”¨äºå‚è€ƒ (Keep CV results for reference)
            if 'cross_validation' in result:
                cv_data = result['cross_validation']
                for target, cv_result in cv_data.items():
                    if isinstance(cv_result, dict):
                        row[f'{target}_CV_R2_mean'] = cv_result.get('r2_mean', 'N/A')
                        row[f'{target}_CV_R2_std'] = cv_result.get('r2_std', 'N/A')
            
            comparison_data.append(row)
        
        # ä¿å­˜å¯¹æ¯”è¡¨æ ¼
        if comparison_data:
            df = pd.DataFrame(comparison_data)
            csv_path = os.path.join(self.comparison_dir, "model_comparison_results.csv")
            df.to_csv(csv_path, index=False)
            print(f"[ç»“æœ] å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
            
            # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            plot_results = create_model_comparison_plots(df, self.comparison_dir)
            if plot_results.get('comprehensive_plot'):
                print(f"[å›¾è¡¨] ç»¼åˆå¯¹æ¯”å›¾è¡¨å·²ç”Ÿæˆ")
                print(f"   - åŒ…å« MAEã€RÂ²ã€RMSE å¯¹æ¯”å’Œç»¼åˆæ’å")
                print(f"   - æ”¯æŒ {plot_results.get('targets_count', 0)} ä¸ªç›®æ ‡å˜é‡")
                print(f"   - å¯¹æ¯” {plot_results.get('models_count', 0)} ä¸ªæ¨¡å‹")

            if plot_results.get('individual_plots'):
                print(f"[å›¾è¡¨] å•æŒ‡æ ‡å¯¹æ¯”å›¾è¡¨: {len(plot_results['individual_plots'])} ä¸ª")
        
        # ä¿å­˜è¯¦ç»†é‡å¤å®éªŒç»“æœ (Detailed Repeats CSV)
        detailed_data = []
        for model_name, result in self.results.items():
            if 'error' in result or 'repeat_results' not in result:
                continue
            
            for i, rep_res in enumerate(result['repeat_results']):
                d_row = {'Model': model_name, 'Repeat': i + 1}
                
                # æå–å„é¡¹æŒ‡ (Extract metrics)
                if 'final_evaluation' in rep_res:
                    for k, v in rep_res['final_evaluation'].items():
                        if isinstance(v, (int, float, str)):
                            d_row[k] = v
                            
                elif 'evaluation' in rep_res and 'test_metrics' in rep_res['evaluation']:
                     # Fallback
                     for target, metrics in rep_res['evaluation']['test_metrics'].items():
                         for m_k, m_v in metrics.items():
                             d_row[f"{target}_{m_k}"] = m_v
                             
                detailed_data.append(d_row)
        
        if detailed_data:
            detailed_df = pd.DataFrame(detailed_data)
            # Sort columns
            cols = ['Model', 'Repeat'] + [c for c in detailed_df.columns if c not in ['Model', 'Repeat']]
            detailed_df = detailed_df[cols]
            
            detailed_path = os.path.join(self.comparison_dir, "model_comparison_detailed_repeats.csv")
            detailed_df.to_csv(detailed_path, index=False)
            print(f"[ç»“æœ] è¯¦ç»†é‡å¤å®éªŒç»“æœå·²ä¿å­˜åˆ°: {detailed_path}")

        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._create_text_report()
    
    def _create_text_report(self):
        """åˆ›å»ºæ–‡æœ¬æ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š"""
        report_path = os.path.join(self.comparison_dir, "comparison_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š\n")
            f.write("Machine Learning Model Comparison Report\n")
            f.write("=" * 80 + "\n\n")
            
            for model_name, result in self.results.items():
                f.write(f"æ¨¡å‹ / Model: {model_name.upper()}\n")
                f.write("-" * 40 + "\n")
                
                if 'error' in result:
                    f.write(f"âŒ è®­ç»ƒå¤±è´¥ / Training Failed: {result['error']}\n\n")
                    continue
                
                # å†™å…¥æœ€ç»ˆè¯„ä¼°ç»“æœ (ä¼˜å…ˆä½¿ç”¨final_evaluation)
                if 'final_evaluation' in result:
                    f.write("æµ‹è¯•é›†è¯„ä¼°ç»“æœ / Test Set Evaluation:\n")
                    final_metrics = result['final_evaluation']

                    # æå–ç›®æ ‡å˜é‡å
                    targets = set()
                    for key in final_metrics.keys():
                        if '_test_' in key:
                            parts = key.split('_')
                            if len(parts) >= 4:
                                target_start_idx = parts.index('test') + 1
                                target_name = '_'.join(parts[target_start_idx:-1])
                                targets.add(target_name)

                    for target in sorted(targets):
                        f.write(f"  {target}:\n")
                        r2_key = f"final_model_evaluation_test_{target}_r2"
                        rmse_key = f"final_model_evaluation_test_{target}_rmse"
                        mae_key = f"final_model_evaluation_test_{target}_mae"

                        r2_val = final_metrics.get(r2_key, 'N/A')
                        rmse_val = final_metrics.get(rmse_key, 'N/A')
                        mae_val = final_metrics.get(mae_key, 'N/A')

                        if r2_val != 'N/A':
                            f.write(f"    RÂ² Score: {r2_val:.4f}\n")
                        else:
                            f.write(f"    RÂ² Score: N/A\n")

                        if rmse_val != 'N/A':
                            f.write(f"    RMSE: {rmse_val:.4f}\n")
                        else:
                            f.write(f"    RMSE: N/A\n")

                        if mae_val != 'N/A':
                            f.write(f"    MAE: {mae_val:.4f}\n")
                        else:
                            f.write(f"    MAE: N/A\n")

                # å¤‡ç”¨ï¼šå†™å…¥æ—§æ ¼å¼è¯„ä¼°ç»“æœ
                elif 'evaluation' in result:
                    f.write("æµ‹è¯•é›†è¯„ä¼°ç»“æœ / Test Set Evaluation:\n")
                    eval_data = result['evaluation']
                    if 'test_metrics' in eval_data:
                        for target, metrics in eval_data['test_metrics'].items():
                            f.write(f"  {target}:\n")
                            f.write(f"    RÂ² Score: {metrics.get('r2', 'N/A'):.4f}\n")
                            f.write(f"    RMSE: {metrics.get('rmse', 'N/A'):.4f}\n")
                            f.write(f"    MAE: {metrics.get('mae', 'N/A'):.4f}\n")
                
                # å†™å…¥äº¤å‰éªŒè¯ç»“æœ
                if 'cross_validation' in result:
                    f.write("\näº¤å‰éªŒè¯ç»“æœ / Cross-Validation Results:\n")
                    cv_data = result['cross_validation']
                    for target, cv_result in cv_data.items():
                        if isinstance(cv_result, dict):
                            f.write(f"  {target}:\n")
                            f.write(f"    RÂ² Mean: {cv_result.get('r2_mean', 'N/A'):.4f}\n")
                            f.write(f"    RÂ² Std: {cv_result.get('r2_std', 'N/A'):.4f}\n")

                # å†™å…¥æœ€ä½³å‚æ•°ä¿¡æ¯
                if 'best_params' in result:
                    f.write("\næœ€ä½³è¶…å‚æ•° / Best Hyperparameters:\n")
                    best_params = result['best_params']
                    for param, value in best_params.items():
                        f.write(f"  {param}: {value}\n")

                # ç”Ÿæˆç¬¦åˆè¦æ±‚çš„æ€§èƒ½æ‘˜è¦ (Performance Summary)
                # ä¼˜å…ˆä½¿ç”¨äº¤å‰éªŒè¯ç»“æœè®¡ç®—ä¸ç¡®å®šåº¦ (Use CV results for uncertainty)
                target_stats = {}
                
                if 'cross_validation' in result:
                    cv_data = result['cross_validation']
                    for target, metrics in cv_data.items():
                        if isinstance(metrics, dict):
                            if target not in target_stats:
                                target_stats[target] = {}
                                
                            # Map CV metric names to standard names
                            # CV often has: r2_mean, r2_std, rmse_mean, rmse_std, mae_mean, mae_std
                            for m in ['r2', 'rmse', 'mae', 'mape']:
                                mean_key = f"{m}_mean"
                                std_key = f"{m}_std"
                                
                                if mean_key in metrics:
                                    target_stats[target][m] = {
                                        'mean': metrics[mean_key],
                                        'std': metrics.get(std_key, 0.0)
                                    }

                # å¦‚æœæ²¡æœ‰CVç»“æœï¼Œå°è¯•ä½¿ç”¨Testç»“æœ (Fallback to Test results if no CV)
                if not target_stats and 'final_evaluation' in result:
                    final_metrics = result['final_evaluation']
                    # Parses keys like "final_model_evaluation_test_{Target}_{metric}"
                    for k, v in final_metrics.items():
                        if "_test_" in k:
                            parts = k.split('_')
                            try:
                                test_idx = parts.index('test')
                            except ValueError:
                                continue
                            
                            suffix = parts[-1]
                            # Only handle single values here, as std usually comes from CV or repeats
                            # If individual test result, std is 0
                            if suffix != 'std': 
                                metric = suffix
                                target = "_".join(parts[test_idx+1:-1])
                                
                                if target not in target_stats:
                                    target_stats[target] = {}
                                if metric not in target_stats[target]:
                                    target_stats[target][metric] = {'mean': v, 'std': 0.0}

                if target_stats:
                    f.write("\næ€§èƒ½æ‘˜è¦ / Performance Summary:\n")
                    summary_lines = []
                    for metric in ['r2', 'mae', 'rmse', 'mape']:
                         targets_with_m = [t for t in target_stats if metric in target_stats[t] and 'mean' in target_stats[t][metric]]
                         if not targets_with_m:
                             continue
                         
                         targets_with_m.sort()
                         val_strs = []
                         t_names = []
                         
                         for t in targets_with_m:
                             stats = target_stats[t][metric]
                             m_val = stats.get('mean', 0)
                             s_val = stats.get('std', 0)
                             
                             if metric.lower() == 'r2':
                                 # Format as percentage if it looks like r2 (<=1.0)
                                 # Use absolute value check to allow for negative R2
                                 if abs(m_val) <= 1.0:
                                     val_strs.append(f"{m_val*100:.2f}% (Â±{s_val*100:.2f}%)")
                                 else:
                                     # Already percentage? Unlikely for R2 std implementation usually, strictly assumes 0-1
                                     # But just in case
                                     val_strs.append(f"{m_val:.2f}% (Â±{s_val:.2f}%)")
                             else:
                                 # For MAE/RMSE, use raw values
                                 val_strs.append(f"{m_val:.4f} (Â±{s_val:.4f})")
                             
                             t_names.append(t)
                         
                         if val_strs:
                             if len(val_strs) > 1:
                                 v_join = ", ".join(val_strs[:-1]) + " and " + val_strs[-1]
                                 n_join = ", ".join(t_names[:-1]) + " and " + t_names[-1]
                             else:
                                 v_join = val_strs[0]
                                 n_join = t_names[0]
                             
                             summary_lines.append(f"{metric.upper()} of {v_join} for {n_join}, respectively.")

                    if summary_lines:
                        f.write("\n".join(summary_lines) + "\n")
                
                f.write("\n" + "=" * 80 + "\n\n")
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
