"""
ç®€åŒ–çš„æ¨¡å‹å¯¹æ¯”è„šæœ¬
Simplified Model Comparison Script

æä¾›æ¨¡å‹å¯¹æ¯”çš„æ ¸å¿ƒåŠŸèƒ½æ¥å£
Provides core model comparison functionality interface
"""

import sys
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆå½“ä½œä¸ºæ¨¡å—è¿è¡Œæ—¶ï¼‰
    from .model_comparator import ModelComparator
    from .config_manager import (
        get_config_parser,
        load_config_from_file,
        merge_configs,
        get_default_config,
        validate_config
    )
except ImportError:
    # å°è¯•ç›´æ¥å¯¼å…¥ï¼ˆå½“ç›´æ¥è¿è¡Œæ—¶ï¼‰
    try:
        from model_comparator import ModelComparator
        from config_manager import (
            get_config_parser,
            load_config_from_file,
            merge_configs,
            get_default_config,
            validate_config
        )
    except ImportError:
        # ä½¿ç”¨å®Œæ•´è·¯å¾„å¯¼å…¥
        from src.models.ml_train_eval_pipeline.model_comparator import ModelComparator
        from src.models.ml_train_eval_pipeline.config_manager import (
            get_config_parser,
            load_config_from_file,
            merge_configs,
            get_default_config,
            validate_config
        )


def run_model_comparison(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¿è¡Œæ¨¡å‹å¯¹æ¯”çš„æ ¸å¿ƒå‡½æ•°
    Core function to run model comparison
    
    Args:
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„å‚æ•°
        
    Returns:
        åŒ…å«æ‰€æœ‰æ¨¡å‹ç»“æœçš„å­—å…¸
    """
    print("ğŸš€ æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹æ¯” / ML Model Comparison")
    print("=" * 80)
    
    # éªŒè¯é…ç½®
    validate_config(config)
    
    # æå–æ¨¡å‹å¯¹æ¯”å‚æ•°
    models_to_compare = config['models']
    use_optuna = config.get('use_optuna', False)
    n_trials = config.get('n_trials', 20)
    
    print(f"ğŸ“‹ å°†å¯¹æ¯”ä»¥ä¸‹æ¨¡å‹: {', '.join(models_to_compare)}")
    print(f"ğŸ”§ ä½¿ç”¨Optunaä¼˜åŒ–: {'æ˜¯' if use_optuna else 'å¦'}")
    if use_optuna:
        print(f"ğŸ”¢ Optunaè¯•éªŒæ¬¡æ•°: {n_trials}")
    
    # åˆ›å»ºæ¨¡å‹å¯¹æ¯”å™¨
    comparator = ModelComparator(config)
    
    # å¼€å§‹å¯¹æ¯”
    results = comparator.compare_models(
        models_to_compare=models_to_compare,
        use_optuna=use_optuna,
        n_trials=n_trials
    )
    
    print(f"\n[å®Œæˆ] æ¨¡å‹å¯¹æ¯”å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {comparator.comparison_dir}")
    print("\n[ç»“æœ] æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶è·å–è¯¦ç»†ç»“æœ:")
    print("  - model_comparison_results.csv: å¯¹æ¯”è¡¨æ ¼")
    print("  - model_comparison_plots.png: å¯è§†åŒ–å›¾è¡¨")
    print("  - comparison_report.txt: è¯¦ç»†æ–‡æœ¬æŠ¥å‘Š")
    print("  - å„æ¨¡å‹å­ç›®å½•: åŒ…å«æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†è®­ç»ƒç»“æœ")
    
    return results


def main():
    """
    ä¸»å‡½æ•° - è§£æå‚æ•°å¹¶è¿è¡Œæ¨¡å‹å¯¹æ¯”
    Main function - Parse arguments and run model comparison
    """
    parser = get_config_parser()
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = {}
        
        # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œå…ˆåŠ è½½æ–‡ä»¶é…ç½®
        if args.config:
            config = load_config_from_file(args.config)
        
        # å°†å‘½ä»¤è¡Œå‚æ•°è½¬æ¢ä¸ºå­—å…¸
        args_dict = {k: v for k, v in vars(args).items() if v is not None and k != 'config'}
        
        # åˆå¹¶é…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
        if args_dict:
            config = merge_configs(config, args_dict)
        
        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if not config:
            config = get_default_config()
        
        # è¿è¡Œæ¨¡å‹å¯¹æ¯”
        run_model_comparison(config)
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {str(e)}")
        print("\nğŸ’¡ æç¤º:")
        print("- æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("- ç¡®ä¿æ•°æ®æ–‡ä»¶è·¯å¾„å­˜åœ¨")
        print("- ä½¿ç”¨ --help æŸ¥çœ‹å‚æ•°è¯´æ˜")
        sys.exit(1)


if __name__ == '__main__':
    main()
